#!/usr/bin/env python
# -*- coding: utf-8 -*-

import re
import json
import logging
import requests
import urllib3
from urllib.parse import urljoin, urlencode, quote, unquote
from bs4 import BeautifulSoup

from modules.scrapers.base_scraper import BaseScraper

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class FanzaScraper(BaseScraper):
    """FANZAç½‘ç«™çˆ¬è™«ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–FANZAçˆ¬è™«ç±»"""
        super().__init__()
        
        # åŸºç¡€URLè®¾ç½®
        self.base_url = "https://www.dmm.co.jp"
        # æ·»åŠ analyzeå‚æ•°ï¼Œæé«˜æœç´¢ç²¾ç¡®åº¦
        self.search_url_template = "https://www.dmm.co.jp/search/=/searchstr={}/analyze=V1EBAwoQAQcGXQ0OXw4C/"
        
        # è¯¦æƒ…é¡µURLæ¨¡æ¿ - ä¸åŒç±»å‹çš„å•†å“æœ‰ä¸åŒçš„URLè·¯å¾„
        self.detail_url_templates = [
            "https://www.dmm.co.jp/digital/videoa/-/detail/=/cid={}/",  # æ•°å­—ç‰ˆ
            "https://www.dmm.co.jp/mono/dvd/-/detail/=/cid={}/",        # DVDç‰ˆ
            "https://www.dmm.co.jp/digital/videoc/-/detail/=/cid={}/",  # æˆäººåŠ¨ç”»
            "https://www.dmm.co.jp/rental/ppr/-/detail/=/cid={}/"       # ç§Ÿèµç‰ˆ
        ]
        
        # FANZAç‰¹æœ‰è®¾ç½®
        self.cookies = {
            'age_check_done': '1',  # å¹´é¾„ç¡®è®¤
            'locale': 'ja'          # ä½¿ç”¨æ—¥è¯­
        }
        
        # æ›´æ–°User-Agentä¸ºæ›´ç°ä»£çš„ç‰ˆæœ¬
        self.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none'
        })
        
    def create_session(self):
        """åˆ›å»ºä¸€ä¸ªHTTPä¼šè¯ï¼Œé’ˆå¯¹DMMç½‘ç«™ä¼˜åŒ–"""
        session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        session.headers.update(self.headers)
        
        # è®¾ç½®cookies
        for key, value in self.cookies.items():
            session.cookies.set(key, value)
        
        # è®¾ç½®é€‚é…å™¨é…ç½®
        adapter = requests.adapters.HTTPAdapter(
            max_retries=requests.adapters.Retry(
                total=3,
                backoff_factor=1,
                status_forcelist=[500, 502, 503, 504, 520, 521, 522, 523, 524],
                allowed_methods=["HEAD", "GET", "OPTIONS"]
            )
        )
        
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        # SSLè®¾ç½® - å¤„ç†DMMçš„SSLé—®é¢˜
        session.verify = False  # ç¦ç”¨SSLéªŒè¯
        
        return session
    
    def clean_movie_id(self, movie_id, five_digit=False):
        """æ ‡å‡†åŒ–å½±ç‰‡ID
        
        Args:
            movie_id: åŸå§‹å½±ç‰‡ID
            five_digit: æ˜¯å¦æ ¼å¼åŒ–ä¸º5ä½æ•°å­—
            
        Returns:
            tuple: (å‚å•†ä»£å·, æ•°å­—éƒ¨åˆ†, å®Œæ•´ID)
        """
        # æ¸…ç†IDï¼Œæå–å­—æ¯å’Œæ•°å­—éƒ¨åˆ†
        movie_id = movie_id.strip()
        match = re.search(r'([a-zA-Z]+)[-_]?(\d+)', movie_id, re.IGNORECASE)
        
        if not match:
            self.logger.warning(f"æ— æ³•è§£æå½±ç‰‡ID: {movie_id}")
            return None, None, movie_id
        
        # æå–å‚å•†ä»£å·å’Œæ•°å­—éƒ¨åˆ†
        label = match.group(1).upper()
        number = match.group(2)
        
        # æ ¼å¼åŒ–æ•°å­—éƒ¨åˆ†
        if five_digit and len(number) < 5:
            number = number.zfill(5)  # å¡«å……ä¸º5ä½æ•°å­—
        
        # è¿”å›æ ‡å‡†åŒ–ç»“æœ
        clean_id = f"{label}-{number}"
        
        self.logger.debug(f"æ¸…ç†å½±ç‰‡ID: {movie_id} -> {clean_id}")
        return label, number, clean_id
    
    def get_movie_url(self, movie_id):
        """æ„å»ºå½±ç‰‡è¯¦æƒ…é¡µURL
        
        Args:
            movie_id: å½±ç‰‡ID
            
        Returns:
            str: è¯¦æƒ…é¡µURL
        """
        # æ ‡å‡†åŒ–å½±ç‰‡ID
        label, number, clean_id = self.clean_movie_id(movie_id)
        if not label:
            return None
        
        # æ„é€ å¯èƒ½çš„å•†å“ä»£ç (cid)æ ¼å¼
        possible_cids = [
            f"{label.lower()}00{number}",                 # æ ‡å‡†æ ¼å¼ï¼šabc00123
            f"{label.lower()}{number}",                   # ç®€å•æ ¼å¼ï¼šabc123
            f"3{label.lower()}{number}",                  # DVDæ ¼å¼ï¼š3abc123
            f"33{label.lower()}{number}",                 # ç§Ÿèµæ ¼å¼ï¼š33abc123
            f"33{label.lower()}{number}dod",              # DODæ ¼å¼ï¼š33abc123dod
            f"{label.lower()}{number.zfill(5)}"           # äº”ä½æ•°å­—æ ¼å¼ï¼šabc00123
        ]
        
        # å°è¯•æ‰€æœ‰å¯èƒ½çš„URLç»„åˆ
        all_urls = []
        for cid in possible_cids:
            for template in self.detail_url_templates:
                all_urls.append(template.format(cid))
        
        self.logger.info(f"æ„å»ºURL: {movie_id} -> {all_urls[0]}")
        return all_urls[0]  # è¿”å›ç¬¬ä¸€ä¸ªURLä¾›ç›´æ¥è®¿é—®å°è¯•
    
    def get_movie_info(self, movie_id):
        """è·å–å½±ç‰‡ä¿¡æ¯çš„ä¸»å‡½æ•° - è¦†ç›–åŸºç±»æ–¹æ³•ï¼Œå…ˆæœç´¢ï¼Œå†å°è¯•ç›´æ¥URL
        
        Args:
            movie_id: å½±ç‰‡ID
            
        Returns:
            dict: å½±ç‰‡ä¿¡æ¯å­—å…¸ æˆ– Noneï¼ˆå¦‚æœæ‰¾ä¸åˆ°å½±ç‰‡ï¼‰
        """
        self.logger.info(f"è·å–å½±ç‰‡ä¿¡æ¯: {movie_id}")
        
        # 1. é¦–å…ˆå°è¯•æœç´¢
        self.logger.info(f"å°è¯•æœç´¢: {movie_id}")
        url_list = self.search_movie(movie_id)
        
        if url_list:
            # è·å–ç¬¬ä¸€ä¸ªURLï¼ˆé€šå¸¸æ˜¯æœ€åŒ¹é…çš„ç»“æœï¼‰
            url = url_list[0]
            self.logger.info(f"æœç´¢æ‰¾åˆ°è¯¦æƒ…é¡µURL: {url}")
            
            # å¦‚æœæ˜¯ video.dmm.co.jp çš„å®¢æˆ·ç«¯æ¸²æŸ“é¡µé¢ï¼Œç›´æ¥èµ° GraphQL
            if "video.dmm.co.jp" in url:
                content_id = self._extract_content_id_from_video_url(url)
                if content_id:
                    graph_info = self._fetch_video_dmm_content_by_content_id(content_id, movie_id)
                    if graph_info:
                        return graph_info
                # æ— æ³•è§£ææˆ–GraphQLå¤±è´¥åˆ™ç»§ç»­å¸¸è§„æµç¨‹

            # ğŸ¯ ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»åœ¨æœç´¢è¿‡ç¨‹ä¸­éªŒè¯è¿‡æ­¤é¡µé¢
            if hasattr(self, '_verified_page_cache') and url in self._verified_page_cache:
                self.logger.info(f"ä½¿ç”¨å·²éªŒè¯çš„é¡µé¢ç¼“å­˜")
                soup = self._verified_page_cache[url]
            else:
                # è·å–è¯¦æƒ…é¡µå†…å®¹
                soup = self.get_page(url)
            
            if soup:
                self.logger.info(f"æˆåŠŸè·å–è¯¦æƒ…é¡µï¼Œæå–ä¿¡æ¯")
                info = self.extract_info_from_page(soup, movie_id, url)
                if info:
                    return info
        
        # 2. æœç´¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥æ„å»ºURL
        self.logger.info(f"æœç´¢æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•ç›´æ¥è®¿é—®URL")
        url = self.get_movie_url(movie_id)
        
        if url and self.is_valid_url(url):
            self.logger.info(f"å°è¯•ç›´æ¥è®¿é—®URL: {url}")
            soup = self.get_page(url)
            
            if soup:
                self.logger.info(f"ç›´æ¥è®¿é—®æˆåŠŸï¼Œæå–ä¿¡æ¯")
                info = self.extract_info_from_page(soup, movie_id, url)
                if info:
                    return info
        
        # 3. å›é€€ï¼šå°è¯• video.dmm.co.jp çš„ GraphQL æ¥å£ï¼ˆå®¢æˆ·ç«¯æ¸²æŸ“ï¼‰
        self.logger.info("å°è¯•é€šè¿‡ video.dmm.co.jp GraphQL æ¥å£è·å–è¯¦æƒ…")
        graph_info = self._fetch_video_dmm_content(movie_id)
        if graph_info:
            return graph_info

        self.logger.warning(f"æœªæ‰¾åˆ°å½±ç‰‡: {movie_id}")
        return None
    
    def search_movie(self, movie_id):
        """æœç´¢å½±ç‰‡
        
        Args:
            movie_id: å½±ç‰‡ID
            
        Returns:
            list: è¯¦æƒ…é¡µURLåˆ—è¡¨
        """
        # è·å–æ¸…ç†åçš„ID
        label, number, clean_id = self.clean_movie_id(movie_id)
        if not label:
            return []
        
        # å°è¯•å¤šç§æœç´¢æ ¼å¼
        search_terms = [
            clean_id,                          # æ ‡å‡†æ ¼å¼ï¼šABC-123
            f"{label}-{number.zfill(5)}",      # 5ä½æ•°å­—æ ¼å¼ï¼šABC-00123
            f"{label}{number}",                # æ— è¿å­—ç¬¦æ ¼å¼ï¼šABC123
            f"{label}{number.zfill(5)}",       # æ— è¿å­—ç¬¦ä¸”é›¶å¡«å……ï¼šABC000123
            label + number                     # æ— ä»»ä½•åˆ†éš”ï¼šABC123
        ]
        
        all_urls = []
        for term in search_terms:
            encoded_term = quote(term)
            search_url = self.search_url_template.format(encoded_term)
            
            self.logger.info(f"æœç´¢URL: {search_url}")
            
            try:
                session = self.create_session()
                response = session.get(search_url, timeout=15)
                
                if response.status_code != 200:
                    self.logger.warning(f"æœç´¢è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    continue
                    
                # è·å–é¡µé¢å†…å®¹
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è°ƒè¯•ä¿¡æ¯
                page_title = soup.title.text if soup.title else "æ— æ ‡é¢˜"
                self.logger.info(f"é¡µé¢æ ‡é¢˜: {page_title}")
                
                # æå–æœç´¢ç»“æœä¸­çš„è¯¦æƒ…é¡µé“¾æ¥
                urls = self._extract_links_from_search_page(soup, clean_id)
                
                if urls:
                    self.logger.info(f"æœç´¢ '{term}' æ‰¾åˆ° {len(urls)} ä¸ªç»“æœ")
                    all_urls.extend(urls)
                    
                    # ğŸ¯ ä¼˜åŒ–ï¼šæ‰¾åˆ°ç»“æœåç«‹å³å°è¯•è·å–è¯¦æƒ…é¡µä¿¡æ¯
                    self.logger.info(f"æ‰¾åˆ°æœç´¢ç»“æœï¼Œå°è¯•éªŒè¯æœ€ä½³åŒ¹é…çš„è¯¦æƒ…é¡µ...")
                    best_urls = self._find_best_match(urls, movie_id)
                    
                    if best_urls:
                        # å¦‚æœæ˜¯ video.dmm.co.jp é“¾æ¥ï¼Œç›´æ¥è®¤ä¸ºæœ‰æ•ˆï¼ˆåç»­ç”¨GraphQLè·å–ï¼‰
                        test_url = best_urls[0]
                        if "video.dmm.co.jp" in test_url:
                            self.logger.info("æ£€æµ‹åˆ° video.dmm.co.jp é“¾æ¥ï¼Œè·³è¿‡HTMLéªŒè¯ï¼Œç¨åç”¨GraphQLè·å–è¯¦æƒ…")
                            return best_urls
                        
                        # å¦åˆ™æŒ‰åŸæœ‰æ–¹å¼éªŒè¯HTMLè¯¦æƒ…é¡µ
                        self.logger.info(f"éªŒè¯è¯¦æƒ…é¡µ: {test_url}")
                        test_soup = self.get_page(test_url)
                        if test_soup and self._is_valid_detail_page(test_soup):
                            self.logger.info(f"éªŒè¯æˆåŠŸï¼Œä½¿ç”¨æ­¤æœç´¢ç»“æœï¼Œè·³è¿‡åç»­æœç´¢")
                            if not hasattr(self, '_verified_page_cache'):
                                self._verified_page_cache = {}
                            self._verified_page_cache[test_url] = test_soup
                            return best_urls
                        else:
                            self.logger.warning(f"è¯¦æƒ…é¡µéªŒè¯å¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–æœç´¢é¡¹")
                else:
                    self.logger.info(f"æœç´¢ '{term}' æœªæ‰¾åˆ°ç»“æœ")
                    
            except requests.exceptions.RequestException as e:
                self.logger.error(f"æœç´¢è¯·æ±‚å¼‚å¸¸: {str(e)}")
                continue
            except Exception as e:
                self.logger.error(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
                continue
                
        # å¦‚æœæœ‰æœç´¢ç»“æœä½†å‰é¢çš„éªŒè¯éƒ½å¤±è´¥äº†ï¼Œè¿”å›æ‰€æœ‰æ‰¾åˆ°çš„URL
        if all_urls:
            return self._find_best_match(all_urls, movie_id)
        
        return []
    
    def _is_valid_detail_page(self, soup):
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è¯¦æƒ…é¡µ
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            bool: æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è¯¦æƒ…é¡µ
        """
        if not soup:
            return False
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å½±ç‰‡æ ‡é¢˜
        title_tag = soup.find("h1", class_="item-name") or soup.find("h1", id="title")
        if not title_tag:
            return False
            
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¿¡æ¯è¡¨æ ¼
        info_table = soup.find("table", class_="mg-b20") or soup.select_one("table.mg-b12")
        if not info_table:
            return False
            
        return True
    
    def _extract_links_from_search_page(self, soup, movie_id):
        """ä»æœç´¢ç»“æœé¡µæå–è¯¦æƒ…é¡µé“¾æ¥
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            movie_id: å½±ç‰‡ID
            
        Returns:
            list: URLåˆ—è¡¨
        """
        urls = []
        
        # æ£€æŸ¥æ˜¯å¦ç›´æ¥è·³è½¬åˆ°è¯¦æƒ…é¡µ
        if "detail" in soup.title.text.lower():
            self.logger.info("æœç´¢å·²ç›´æ¥è·³è½¬åˆ°è¯¦æƒ…é¡µ")
            current_url = soup.find("link", rel="canonical")
            if current_url and current_url.get("href"):
                urls.append(current_url.get("href"))
                return urls
            return []
        
        # æŸ¥æ‰¾æœç´¢ç»“æœä¸­çš„æ‰€æœ‰é“¾æ¥
        product_links = soup.select("p.tmb a")
        if not product_links:
            # å°è¯•ä¸åŒçš„CSSé€‰æ‹©å™¨
            product_links = soup.select("div.box-image a")
        
        if not product_links:
            # å°è¯•é€šç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è¯¦æƒ…é“¾æ¥
            all_links = soup.find_all("a", href=True)
            product_links = [
                link for link in all_links
                if ("cid=" in link.get("href") or 
                    "video.dmm.co.jp/av/content/?id=" in link.get("href") or
                    "video.dmm.co.jp/amateur/content/?id=" in link.get("href"))
            ]
        
        # å¤„ç†æ‰¾åˆ°çš„é“¾æ¥
        if product_links:
            for link in product_links:
                href = link.get("href")
                if not href:
                    continue
                # æ¥å—ä¸‰ç±»ï¼šcid= è¯¦æƒ…é¡µ æˆ– video.dmm.co.jp çš„ content é¡µé¢ï¼ˆavæˆ–amateurï¼‰
                if ("cid=" in href) or ("video.dmm.co.jp/av/content/?id=" in href) or ("video.dmm.co.jp/amateur/content/?id=" in href):
                    url = href
                    if not url.startswith("http"):
                        url = urljoin(self.base_url, url)
                    urls.append(url)
        
        return urls
    
    def _find_best_match(self, urls, movie_id):
        """ä»URLåˆ—è¡¨ä¸­æ‰¾å‡ºæœ€åŒ¹é…çš„ç»“æœ
        
        Args:
            urls: URLåˆ—è¡¨
            movie_id: å½±ç‰‡ID
            
        Returns:
            list: æ’åºåçš„URLåˆ—è¡¨
        """
        # è·å–æ ‡å‡†åŒ–çš„å½±ç‰‡ID
        label, number, clean_id = self.clean_movie_id(movie_id)
        label_part = label.lower() if label else ""
        number_part = number if number else ""
        
        # å®šä¹‰ä¼˜å…ˆçº§è®¡ç®—å‡½æ•°
        def get_priority(url):
            priority = 0
            
            # 1. ä¼˜å…ˆè€ƒè™‘æ•°å­—ç‰ˆï¼ˆdigital/videoaï¼‰
            if "digital/videoa" in url:
                priority += 100
            # 2. å…¶æ¬¡è€ƒè™‘DVDç‰ˆ
            elif "mono/dvd" in url:
                priority += 50
            # 3. å†æ¬¡è€ƒè™‘åŠ¨ç”»
            elif "digital/videoc" in url:
                priority += 30
            # 4. æœ€åè€ƒè™‘ç§Ÿèµ
            elif "rental" in url:
                priority += 10
                
            # 5. video.dmm.co.jp çš„å†…å®¹é¡µä¼˜å…ˆçº§æœ€é«˜ï¼ˆå¯ç›´æ¥GraphQLï¼‰
            if "video.dmm.co.jp/av/content" in url:
                priority += 1000
            elif "video.dmm.co.jp/amateur/content" in url:
                priority += 1000  # amateurå†…å®¹ä¹Ÿä½¿ç”¨GraphQLï¼Œä¼˜å…ˆçº§ç›¸åŒ
            
            # 5.5. é™ä½monthlyé“¾æ¥çš„ä¼˜å…ˆçº§ï¼ˆæœˆé¢åŠ¨ç”»ï¼Œé€šå¸¸ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼‰
            if "monthly/" in url:
                priority -= 10000  # å¤§å¹…é™ä½ä¼˜å…ˆçº§ï¼Œç¡®ä¿ä½äºvideo.dmm.co.jp

            # 6. æ£€æŸ¥URLä¸­çš„cidåŒ¹é…åº¦
            cid_match = re.search(r'cid=([^/]+)', url, re.IGNORECASE)
            if cid_match:
                cid = cid_match.group(1).lower()
                
                # 6.1 å®Œå…¨åŒ¹é…ï¼šcidä¸clean_idå®Œå…¨ä¸€è‡´ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
                if cid == clean_id.lower():
                    priority += 10000  # æœ€é«˜ä¼˜å…ˆçº§
                    self.logger.info(f"æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„cid: {cid} == {clean_id}")
                
                # 6.1.5 çº¯å‡€åŒ¹é…ï¼šä¼˜å…ˆé€‰æ‹©æ²¡æœ‰åç¼€çš„CIDï¼ˆå¦‚ssni314è€Œä¸æ˜¯ssni314bodï¼‰
                elif label_part and number_part:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯å‡€çš„label+numberæ ¼å¼ï¼ˆæ²¡æœ‰é¢å¤–åç¼€ï¼‰
                    pure_pattern = f"^{label_part}{number_part}$"
                    if re.match(pure_pattern, cid):
                        priority += 8000  # å¾ˆé«˜ä¼˜å…ˆçº§ï¼Œä»…æ¬¡äºå®Œå…¨åŒ¹é…
                        self.logger.info(f"æ‰¾åˆ°çº¯å‡€åŒ¹é…çš„cid: {cid} == {label_part}{number_part}")
                
                # 6.2 ç²¾ç¡®åŒ¹é…ï¼šæ£€æŸ¥labelå’Œnumberæ˜¯å¦éƒ½åŒ¹é…
                if label_part and number_part:
                    # æå–cidä¸­çš„labelå’Œnumberéƒ¨åˆ†
                    # ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ä»¥å¤„ç†æ•°å­—å¼€å¤´çš„CIDï¼ˆå¦‚1sdmf002ï¼‰
                    cid_label_match = re.search(r'([a-z]+)', cid)
                    cid_number_match = re.search(r'[a-z]+(\d+)', cid)
                    
                    if cid_label_match and cid_number_match:
                        cid_label = cid_label_match.group(1)
                        cid_number = cid_number_match.group(1)
                        
                        # è°ƒè¯•ä¿¡æ¯
                        self.logger.info(f"åˆ†æCID: {cid}, æå–çš„label: {cid_label}, number: {cid_number}")
                        self.logger.info(f"ç›®æ ‡label: {label_part}, number: {number_part}")
                        
                        # å¦‚æœlabelå’Œnumberéƒ½åŒ¹é…
                        if cid_label == label_part and cid_number == number_part:
                            priority += 5000  # å¾ˆé«˜ä¼˜å…ˆçº§
                            self.logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…: {cid_label}{cid_number} == {label_part}{number_part}")
                        # å¦‚æœåªæœ‰labelåŒ¹é…
                        elif cid_label == label_part:
                            priority += 1000
                            self.logger.info(f"æ‰¾åˆ°labelåŒ¹é…: {cid_label} == {label_part}")
                        # å¦‚æœåªæœ‰numberåŒ¹é…
                        elif cid_number == number_part:
                            priority += 500
                            self.logger.info(f"æ‰¾åˆ°numberåŒ¹é…: {cid_number} == {number_part}")
                
                # 6.3 éƒ¨åˆ†åŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å‚å•†ä»£å·
                if label_part and label_part in cid:
                    priority += 100
                    
            # 7. æ£€æŸ¥video.dmm.co.jpçš„idå‚æ•°åŒ¹é…åº¦
            video_id_match = re.search(r'[?&]id=([^&#]+)', url, re.IGNORECASE)
            if video_id_match:
                video_id = video_id_match.group(1).lower()
                
                # 7.1 å®Œå…¨åŒ¹é…
                if video_id == clean_id.lower():
                    priority += 10000
                    self.logger.info(f"æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„video id: {video_id} == {clean_id}")
                
                # 7.2 ç²¾ç¡®åŒ¹é…
                elif label_part and number_part:
                    video_label_match = re.search(r'^([a-z]+)', video_id)
                    video_number_match = re.search(r'(\d+)', video_id)
                    
                    if video_label_match and video_number_match:
                        video_label = video_label_match.group(1)
                        video_number = video_number_match.group(1)
                        
                        if video_label == label_part and video_number == number_part:
                            priority += 5000
                            self.logger.info(f"æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„video id: {video_label}{video_number} == {label_part}{number_part}")
                        elif video_label == label_part:
                            priority += 1000
                        elif video_number == number_part:
                            priority += 500
                    
            return priority
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_urls = sorted(urls, key=get_priority, reverse=True)
        
        # è®°å½•æ’åºç»“æœç”¨äºè°ƒè¯•
        self.logger.info(f"æœç´¢ç»“æœæ’åºï¼ˆå‰10ä¸ªï¼‰:")
        for i, url in enumerate(sorted_urls[:10]):
            priority = get_priority(url)
            self.logger.info(f"  {i+1}. ä¼˜å…ˆçº§={priority}, URL={url}")
        
        # ç‰¹åˆ«æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡CID
        target_cids = [f"cid={clean_id.lower()}", f"cid=1{label_part}{number_part}"]
        for target_cid in target_cids:
            matching_urls = [url for url in sorted_urls if target_cid in url.lower()]
            if matching_urls:
                self.logger.info(f"æ‰¾åˆ°ç›®æ ‡CID {target_cid} çš„URL: {matching_urls[0]}")
                # æ£€æŸ¥è¿™ä¸ªURLçš„ä¼˜å…ˆçº§
                target_priority = get_priority(matching_urls[0])
                self.logger.info(f"ç›®æ ‡URLä¼˜å…ˆçº§: {target_priority}")
            else:
                self.logger.info(f"æœªæ‰¾åˆ°ç›®æ ‡CID {target_cid} çš„URL")
        
        # å»é™¤é‡å¤URL
        unique_urls = []
        for url in sorted_urls:
            if url not in unique_urls:
                unique_urls.append(url)
                
        return unique_urls
    
    def _convert_to_high_quality_image(self, img_url):
        """å°†ç¼©ç•¥å›¾URLè½¬æ¢ä¸ºé«˜è´¨é‡å¤§å›¾URL
        
        Args:
            img_url: ç¼©ç•¥å›¾URL
            
        Returns:
            str: é«˜è´¨é‡å›¾ç‰‡URL
            
        URLæ¨¡å¼è¯´æ˜ï¼š
        - å°é¢å°å›¾: https://pics.dmm.co.jp/mono/movie/adult/1start310v/1start310vps.jpg
        - å°é¢å¤§å›¾: https://pics.dmm.co.jp/mono/movie/adult/1start310v/1start310vpl.jpg
        - é¢„è§ˆå°å›¾: https://pics.dmm.co.jp/digital/video/1start310v/1start310v-1.jpg
        - é¢„è§ˆå¤§å›¾: https://pics.dmm.co.jp/digital/video/1start310v/1start310vjp-1.jpg
        """
        if not img_url:
            return None
        
        # æ¨¡å¼1: å°é¢å›¾è½¬æ¢ (ps.jpg -> pl.jpg)
        if 'ps.jpg' in img_url:
            return img_url.replace('ps.jpg', 'pl.jpg')
        
        # æ¨¡å¼2: å°é¢å›¾è½¬æ¢ (pt.jpg -> pl.jpg)
        if 'pt.jpg' in img_url:
            return img_url.replace('pt.jpg', 'pl.jpg')
        
        # æ¨¡å¼3: é¢„è§ˆå›¾è½¬æ¢ (ä¾‹å¦‚: 1start310v-1.jpg -> 1start310vjp-1.jpg)
        # åŒ¹é…æ¨¡å¼: /video/ç•ªå·/ç•ªå·-æ•°å­—.jpg
        import re
        match = re.search(r'/video/([^/]+)/\1-(\d+)\.jpg', img_url)
        if match:
            video_id = match.group(1)
            num = match.group(2)
            # æ„å»ºå¤§å›¾URL
            return img_url.replace(f'{video_id}-{num}.jpg', f'{video_id}jp-{num}.jpg')

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ¨¡å¼ï¼Œè¿”å›åŸURL
        return img_url

    @staticmethod
    def _extract_text_with_line_breaks(element) -> str:
        if element is None:
            return ""

        if hasattr(element, "get_text"):
            raw_text = element.get_text(separator="\n")
        else:
            raw_text = str(element)

        if not raw_text:
            return ""

        normalized = raw_text.replace("\r\n", "\n").replace("\r", "\n")
        raw_lines = [line.strip() for line in normalized.split("\n")]

        # å»æ‰é¦–å°¾çº¯ç©ºè¡Œï¼Œé¿å…å‰åå¤šå‡ºæ¥çš„ç©ºç™½æ®µ
        while raw_lines and raw_lines[0] == "":
            raw_lines.pop(0)
        while raw_lines and raw_lines[-1] == "":
            raw_lines.pop()

        # è§„åˆ™ï¼š
        # - æ™®é€šæƒ…å†µä¸‹ï¼Œç”±äº HTML ç¼©è¿›ï¼Œæ¯ä¸€è¡Œæ–‡æœ¬åé¢ä¼šè·Ÿä¸€ä¸ªå•ç‹¬çš„ç©ºè¡Œï¼Œæˆ‘ä»¬è¦å»æ‰è¿™äº›å•ç©ºè¡Œï¼›
        # - ä½†åƒ DMM ç®€ä»‹é‡Œé‚£ç§è¿ç»­ <br><br> çš„åœ°æ–¹ï¼Œä¼šå½¢æˆ 2 ä¸ªä»¥ä¸Šè¿ç»­ç©ºè¡Œï¼Œè¿™é‡Œä¿ç•™ä¸€ä¸ªï¼Œ
        #   ä½œä¸ºâ€œæ®µè½ä¹‹é—´ç©ºä¸€è¡Œâ€çš„æ•ˆæœã€‚
        lines: list[str] = []
        blank_run = 0
        for line in raw_lines:
            if line == "":
                blank_run += 1
                continue
            # å¤„ç†ä¹‹å‰ç´¯è®¡çš„ç©ºè¡Œ
            if blank_run >= 2 and lines:
                # å¤šä¸ªè¿ç»­ç©ºè¡Œ => æ®µè½åˆ†éš”ï¼Œä¿ç•™ä¸€ä¸ªç©ºè¡Œ
                lines.append("")
            # å¦‚æœ blank_run == 1ï¼Œåˆ™è®¤ä¸ºæ˜¯ç¼©è¿›å™ªå£°ï¼Œç›´æ¥ä¸¢å¼ƒ
            blank_run = 0
            lines.append(line)

        return "\n".join(lines)

    @staticmethod
    def _remove_advertisement_from_text(text: str) -> str:
        """
        ä»ç®€ä»‹æ–‡æœ¬ä¸­ç§»é™¤å¸¸è§çš„å¹¿å‘Š/è¯´æ˜è¡Œï¼Œæ¯”å¦‚ã€Œã‚³ãƒ³ãƒ“ãƒ‹å—å–ã€ã€Œè©³ã—ãã¯ã“ã¡ã‚‰ã€ç­‰ã€‚
        åªåœ¨æ•´è¡ŒåŒ…å«è¿™äº›å…³é”®è¯æ—¶æ‰åˆ é™¤ï¼Œå°½é‡é¿å…è¯¯ä¼¤æ­£å¸¸å†…å®¹ã€‚
        """
        if not text:
            return text

        ad_keywords = [
            "ã‚³ãƒ³ãƒ“ãƒ‹å—å–",
            "è©³ã—ãã¯ã“ã¡ã‚‰",
            "å¯¾è±¡å•†å“ã§ã™",
            "æ³¨æ–‡æ–¹æ³•",
            "é€æ–™ç„¡æ–™",
            "ãƒã‚¤ãƒ³ãƒˆ",
            "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³",
            "ã‚»ãƒƒãƒˆå•†å“",
        ]

        lines = text.split("\n")
        filtered_lines = [
            line for line in lines
            if line.strip() and not any(k in line for k in ad_keywords)
        ]

        return "\n".join(filtered_lines).strip()
    
    def extract_info_from_page(self, soup, movie_id, url):
        """ä»é¡µé¢æå–å½±ç‰‡ä¿¡æ¯
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            movie_id: å½±ç‰‡ID
            url: é¡µé¢URL
            
        Returns:
            dict: å½±ç‰‡ä¿¡æ¯å­—å…¸
        """
        if not soup:
            return None
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„è¯¦æƒ…é¡µ
        title_tag = soup.find("h1", class_="item-name") or soup.find("h1", id="title")
        if not title_tag:
            self.logger.warning("é¡µé¢ä¸æ˜¯æœ‰æ•ˆçš„å½±ç‰‡è¯¦æƒ…é¡µ")
            return None
            
        # 1. åˆå§‹åŒ–ç»“æœå­—å…¸
        result = {
            "source": "fanza",
            "id": movie_id,
            "url": url
        }
        
        # 2. æå–æ ‡é¢˜
        title = title_tag.text.strip()
        result["title"] = title
        
        # 3. æå–è¯¦æƒ…ä¿¡æ¯è¡¨æ ¼
        info_table = soup.find("table", class_="mg-b20") or soup.select_one("table.mg-b12")
        
        if not info_table:
            self.logger.warning("æœªæ‰¾åˆ°ä¿¡æ¯è¡¨æ ¼")
            # ä¿å­˜é¡µé¢ä»¥ä¾›è°ƒè¯•
            self.save_debug_file(str(soup), f"debug_fanza_{movie_id}.html")
            return None
        
        # æå–è¡¨æ ¼ä¸­çš„ä¿¡æ¯
        info_rows = info_table.find_all("tr")
        
        for row in info_rows:
            # è·å–æ ‡ç­¾å’Œå€¼
            label_tag = row.find("td", class_="nw") or row.find("td", width="100")
            if not label_tag:
                continue
                
            label = label_tag.text.strip()
            value_tag = label_tag.find_next("td")
            if not value_tag:
                continue
                
            # æ ¹æ®æ ‡ç­¾è§£æä¸åŒç±»å‹çš„ä¿¡æ¯
            if "å•†å“ç™ºå£²æ—¥" in label or "é…ä¿¡é–‹å§‹æ—¥" in label or "ç™ºå£²æ—¥" in label:
                # å‘è¡Œæ—¥æœŸ
                date_text = value_tag.text.strip().replace("/", "-")
                result["release_date"] = date_text
                
            elif "åéŒ²æ™‚é–“" in label or "æ™‚é–“" in label:
                # æ—¶é•¿
                duration_text = value_tag.text.strip()
                duration_match = re.search(r'(\d+)', duration_text)
                if duration_match:
                    result["duration"] = duration_match.group(1) + "åˆ†é’Ÿ"
                    
            elif "å‡ºæ¼”è€…" in label or "å¥³å„ª" in label:
                # æ¼”å‘˜ï¼ˆåŒ…å«åç§°å’Œå¤´åƒï¼‰
                actresses = []
                actress_with_images = []
                actress_links = value_tag.find_all("a")
                if actress_links:
                    for link in actress_links:
                        name = link.text.strip()
                        if name and name != "ï¼š":
                            actresses.append(name)
                            # å°è¯•è·å–æ¼”å‘˜IDä»¥æ„å»ºå¤´åƒURL
                            actress_id = None
                            href = link.get("href")
                            if href:
                                # ä»URLä¸­æå–æ¼”å‘˜IDï¼š/mono/person/-/id=9999/
                                id_match = re.search(r'/id=(\d+)/', href)
                                if id_match:
                                    actress_id = id_match.group(1)
                            
                            # æ„å»ºæ¼”å‘˜ä¿¡æ¯ï¼ˆåŒ…å«å¤´åƒURLï¼‰
                            actress_info = {"name": name}
                            if actress_id:
                                # DMMæ¼”å‘˜å¤´åƒURLæ ¼å¼ï¼šhttps://pics.dmm.co.jp/mono/actjpgs/[actress_id].jpg
                                actress_info["id"] = actress_id
                                actress_info["avatar"] = f"https://pics.dmm.co.jp/mono/actjpgs/{actress_id}.jpg"
                            actress_with_images.append(actress_info)
                else:
                    # æ— é“¾æ¥æ—¶ç›´æ¥è·å–æ–‡æœ¬
                    name = value_tag.text.strip()
                    if name and name != "ï¼š" and name != "----":
                        actresses.append(name)
                        actress_with_images.append({"name": name})
                        
                if actresses:
                    result["actresses"] = actresses
                    result["actress_details"] = actress_with_images
                    
            elif "ç›£ç£" in label:
                # å¯¼æ¼”
                director = value_tag.text.strip()
                if director and director != "----":
                    result["director"] = director
                    
            elif "ã‚·ãƒªãƒ¼ã‚º" in label:
                # ç³»åˆ—
                series = value_tag.text.strip()
                if series and series != "----":
                    result["series"] = series
                    
            elif "ãƒ¡ãƒ¼ã‚«ãƒ¼" in label:
                # åˆ¶ä½œå•†
                maker = value_tag.text.strip()
                if maker and maker != "----":
                    result["maker"] = maker
                    
            elif "ãƒ¬ãƒ¼ãƒ™ãƒ«" in label:
                # å‘è¡Œå•†
                label_text = value_tag.text.strip()
                if label_text and label_text != "----":
                    result["label"] = label_text
                    
            elif "å“ç•ª" in label or "å“ç•ªï¼š" in label:
                # å“ç•ªï¼ˆäº§å“ä»£ç ï¼‰
                product_code = value_tag.text.strip()
                if product_code and product_code != "----":
                    result["product_code"] = product_code
                    
            elif "ã‚¸ãƒ£ãƒ³ãƒ«" in label or "ã‚«ãƒ†ã‚´ãƒª" in label:
                # ç±»å‹/æ ‡ç­¾
                genres = []
                genre_links = value_tag.find_all("a")
                if genre_links:
                    for link in genre_links:
                        genre = link.text.strip()
                        if genre and genre != "ï¼š":
                            genres.append(genre)
                else:
                    # æ— é“¾æ¥æ—¶ç›´æ¥è·å–æ–‡æœ¬
                    genre_text = value_tag.text.strip()
                    if genre_text and genre_text != "ï¼š" and genre_text != "----":
                        genres = [g.strip() for g in re.split(r'[ã€,ã€]', genre_text) if g.strip()]
                        
                if genres:
                    result["genres"] = genres
        
        # 4. æå–å°é¢å›¾
        cover_img = soup.select_one("#sample-video img") or soup.select_one(".item-image img")
        if cover_img:
            img_url = cover_img.get("src") or cover_img.get("data-src")
            if img_url:
                # è½¬æ¢ä¸ºé«˜è´¨é‡å›¾ç‰‡
                high_quality_url = self._convert_to_high_quality_image(img_url)
                result["cover"] = high_quality_url
                
        # 5. æå–é¢„è§ˆå›¾
        thumbnails = []
        # ä¼˜å…ˆæŸ¥æ‰¾ #sample-image-block ä¸­çš„å›¾ç‰‡
        thumbnail_links = soup.select("#sample-image-block img")
        if not thumbnail_links:
            thumbnail_links = soup.select(".position-relative.detail-cap a img")
        
        if thumbnail_links:
            for img in thumbnail_links:
                # ä¼˜å…ˆä½¿ç”¨ data-lazy å±æ€§ï¼ˆæ‡’åŠ è½½ï¼‰ï¼Œå…¶æ¬¡æ˜¯ data-srcï¼Œæœ€åæ˜¯ src
                img_url = img.get("data-lazy") or img.get("data-src") or img.get("src")
                
                # è¿‡æ»¤æ‰å ä½å›¾å’Œæ— æ•ˆå›¾ç‰‡
                if not img_url or img_url.endswith("noimage.jpg") or "dummy_ps.gif" in img_url:
                    continue
                
                # è½¬æ¢å°å›¾ä¸ºå¤§å›¾
                high_quality_url = self._convert_to_high_quality_image(img_url)
                if high_quality_url:
                    thumbnails.append(high_quality_url)
                    
        if thumbnails:
            result["thumbnails"] = thumbnails
            # å¦‚æœæ²¡æœ‰å°é¢å›¾ï¼Œä½¿ç”¨ç¬¬ä¸€å¼ é¢„è§ˆå›¾ä½œä¸ºå°é¢
            if not result.get("cover") and thumbnails:
                result["cover"] = thumbnails[0]
            
        # 6. æå–è¯„åˆ†
        rating_element = soup.select_one(".d-review__average") or soup.select_one(".c-review__average") or soup.select_one(".c-rating-v2__average")
        if rating_element:
            rating_text = rating_element.text.strip()
            rating_match = re.search(r'([\d\.]+)', rating_text)
            if rating_match:
                result["rating"] = rating_match.group(1)
                
        # 7. æå–ç®€ä»‹ï¼ˆæ›´ç²¾å‡†ï¼šä¼˜å…ˆæå–æ­£æ–‡æ®µè½ï¼Œè¿‡æ»¤å¹¿å‘Š/è¯´æ˜å—ï¼‰
        summary_text = None

        try:
            # 7.1 ä¼˜å…ˆé€‰æ‹© page-detail åŒºåŸŸå†…çš„æ­£æ–‡æ®µè½ï¼Œè¿‡æ»¤å¹¿å‘Š/è¯´æ˜
            detail_root = soup.select_one("div.page-detail")
            if detail_root:
                paragraph_candidates = detail_root.select("div.mg-b20.lh4 p, p.mg-b20")
            else:
                paragraph_candidates = soup.select("div.mg-b20.lh4 p, p.mg-b20")
            cleaned_candidates = []

            # å®šä¹‰éœ€è¿‡æ»¤çš„å…³é”®è¯ï¼ˆå¸¸è§å¹¿å‘Š/è¯´æ˜ç”¨è¯­ï¼‰
            ad_keywords = [
                "ç‰¹å…¸", "ã‚»ãƒƒãƒˆå•†å“", "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³", "ã‚ªãƒ•", "ã‚»ãƒ¼ãƒ«", "è©³ã—ãã¯ã“ã¡ã‚‰", "ã‚³ãƒ³ãƒ“ãƒ‹å—å–", "æ³¨æ–‡æ–¹æ³•", "é€æ–™ç„¡æ–™", "ãƒã‚¤ãƒ³ãƒˆ"
            ]

            for p in paragraph_candidates:
                text = self._extract_text_with_line_breaks(p)
                if not text:
                    continue
                # è¿‡æ»¤åŒ…å«å¹¿å‘Šå…³é”®è¯æˆ–è¿‡çŸ­çš„æ®µè½
                if any(k in text for k in ad_keywords):
                    continue
                if len(text) < 50:
                    continue
                # è¿‡æ»¤å¤„äºå¹¿å‘Šè¯´æ˜å®¹å™¨å†…çš„æ®µè½ï¼ˆå¦‚ .d-boxother æˆ– .mg-t20ï¼‰
                parent_classes = " ".join(p.parent.get("class", [])) if p.parent else ""
                if "d-boxother" in parent_classes or "mg-t20" in parent_classes:
                    continue
                cleaned_candidates.append(text)

            # ä¿ç•™æ‰€æœ‰æœ‰æ•ˆçš„æ®µè½ï¼Œä¿æŒåˆ†æ®µç»“æ„
            if cleaned_candidates:
                # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆä¿ç•™è¾ƒé•¿çš„æ®µè½ï¼Œä½†ä¿ç•™æ‰€æœ‰æœ‰æ•ˆæ®µè½
                cleaned_candidates.sort(key=len, reverse=True)
                # å¦‚æœåªæœ‰ä¸€ä¸ªæ®µè½ï¼Œç›´æ¥ä½¿ç”¨
                if len(cleaned_candidates) == 1:
                    summary_text = cleaned_candidates[0]
                else:
                    # å¤šä¸ªæ®µè½æ—¶ï¼Œç”¨åŒæ¢è¡Œç¬¦åˆ†éš”ï¼Œä¿æŒåˆ†æ®µ
                    summary_text = "\n\n".join(cleaned_candidates)

            # 7.2 å…œåº•ï¼šå¦‚æœæ²¡æ‹¿åˆ°ï¼Œé€€å›æ—§çš„é€‰æ‹©å™¨
            if not summary_text:
                description_element = soup.select_one("#introduction-text") or soup.select_one(".mg-b20.lh4")
                if description_element:
                    paras = [self._extract_text_with_line_breaks(t) for t in description_element.select("p")]
                    paras = [t for t in paras if t and len(t) >= 50 and not any(k in t for k in ad_keywords)]
                    if paras:
                        # ä¿æŒåˆ†æ®µç»“æ„
                        if len(paras) == 1:
                            summary_text = paras[0]
                        else:
                            summary_text = "\n\n".join(paras)
                    else:
                        summary_text = self._extract_text_with_line_breaks(description_element)
            # 7.3 å†å…œåº•ï¼šmeta æè¿°
            if not summary_text:
                og_desc = soup.find("meta", attrs={"property": "og:description"})
                if og_desc and og_desc.get("content"):
                    summary_text = og_desc.get("content").strip()
            if not summary_text:
                meta_desc = soup.find("meta", attrs={"name": "description"})
                if meta_desc and meta_desc.get("content"):
                    summary_text = meta_desc.get("content").strip()
        except Exception:
            # å‡ºé”™æ—¶é€€å›æœ€åˆç­–ç•¥ï¼Œä¿è¯ä¸å½±å“æ•´ä½“æŠ“å–
            description_element = soup.select_one("#introduction-text") or soup.select_one(".mg-b20.lh4")
            if description_element:
                summary_text = self._extract_text_with_line_breaks(description_element)

        if summary_text:
            # æœ€åå†åšä¸€å±‚å¹¿å‘Šè¡Œè¿‡æ»¤ï¼Œé˜²æ­¢å…œåº•ç­–ç•¥æŠŠã€Œã‚³ãƒ³ãƒ“ãƒ‹å—å–ã€ç­‰è¯´æ˜æ–‡å­—å¸¦è¿›æ¥
            summary_text = self._remove_advertisement_from_text(summary_text)
            if summary_text:
                result["summary"] = summary_text

        # 8. æå–é›‘èªŒæ²è¼‰ã‚³ãƒ¡ãƒ³ãƒˆ/AVãƒ©ã‚¤ã‚¿ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆä½œä¸ºæ‘˜è¦çš„è¡¥å……æ®µè½ï¼‰
        try:
            journal_comment = soup.select_one("div.journal-comment")
            if journal_comment:
                # æå–æ ‡é¢˜ï¼ˆdtï¼‰å’Œå†…å®¹ï¼ˆddï¼‰
                dt = journal_comment.select_one("dt")
                dd = journal_comment.select_one("dd")
                if dt and dd:
                    title = dt.get_text().strip()
                    content = dd.get_text().strip()
                    if title and content:
                        # å°†è¯„ä»·å†…å®¹ä½œä¸ºæ‘˜è¦çš„è¡¥å……æ®µè½ï¼Œä¿æŒåˆ†æ®µç»“æ„
                        if "summary" in result:
                            # å¦‚æœå·²æœ‰æ‘˜è¦ï¼Œæ·»åŠ æ¢è¡Œç¬¦ä¿æŒåˆ†æ®µ
                            result["summary"] += f"\n\nã€{title}ã€‘\n{content}"
                        else:
                            # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œç›´æ¥ä½¿ç”¨è¯„ä»·å†…å®¹
                            result["summary"] = f"ã€{title}ã€‘\n{content}"
        except Exception:
            # å‡ºé”™æ—¶å¿½ç•¥ï¼Œä¸å½±å“æ•´ä½“æŠ“å–
            pass

        # 9. æå–ç”¨æˆ·è¯„ä»·ï¼ˆUser Reviewsï¼‰- ä»ä¸“é—¨çš„è¯„ä»·é¡µé¢è·å–
        try:
            user_reviews = self._fetch_user_reviews_from_review_page(movie_id, result)
            if user_reviews:
                result["user_reviews"] = user_reviews
                
                # åŒæ—¶å°†ç”¨æˆ·è¯„ä»·æ·»åŠ åˆ°æ‘˜è¦ä¸­ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                review_text = ""
                for review in user_reviews:
                    if review.get("title") and review.get("comment"):
                        review_text += f"\n\nã€{review['title']}ã€‘\n{review['comment']}"
                    elif review.get("comment"):
                        review_text += f"\n\nã€ç”¨æˆ·è¯„ä»·ã€‘\n{review['comment']}"
                
                if review_text and "summary" in result:
                    result["summary"] += review_text
                elif review_text:
                    result["summary"] = review_text.strip()
                    
        except Exception:
            # å‡ºé”™æ—¶å¿½ç•¥ï¼Œä¸å½±å“æ•´ä½“æŠ“å–
            pass
    
        return result 

    def _fetch_user_reviews_from_review_page(self, movie_id, movie_data):
        """ä»ä¸“é—¨çš„ç”¨æˆ·è¯„ä»·é¡µé¢è·å–ç”¨æˆ·è¯„ä»·ï¼ˆæ”¯æŒå¤šé¡µçˆ¬å–ï¼‰"""
        try:
            # ä»å½±ç‰‡æ•°æ®ä¸­æå–cid
            cid = None
            if "url" in movie_data and "cid=" in movie_data["url"]:
                import re
                cid_match = re.search(r'cid=([^/&]+)', movie_data["url"])
                if cid_match:
                    cid = cid_match.group(1)
            
            # å¦‚æœæ²¡æœ‰cidï¼Œå°è¯•ä»movie_idæ„é€ 
            if not cid:
                # å°è¯•ä»movie_idæ„é€ cidï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ˜ å°„é€»è¾‘ï¼‰
                cid = movie_id.lower()
            
            if not cid:
                self.logger.warning(f"æ— æ³•è·å–cidæ¥æ„å»ºè¯„ä»·é¡µé¢URL: {movie_id}")
                return []
            
            # æ„å»ºè¯„ä»·é¡µé¢URL
            base_review_url = f"https://www.dmm.co.jp/mono/dvd/-/detail/review/=/cid={cid}/"
            self.logger.info(f"å°è¯•è·å–ç”¨æˆ·è¯„ä»·: {base_review_url}")
            
            # è·å–æ‰€æœ‰é¡µé¢çš„è¯„ä»·
            all_user_reviews = []
            page = 1
            max_pages = 10  # é™åˆ¶æœ€å¤§é¡µæ•°ï¼Œé¿å…æ— é™å¾ªç¯
            session = self.create_session()
            
            while page <= max_pages:
                # æ„å»ºå½“å‰é¡µé¢çš„URL
                if page == 1:
                    page_url = base_review_url
                else:
                    page_url = f"{base_review_url}?paging={page}&sort=value_desc#review_anchor"
                
                self.logger.info(f"è·å–ç¬¬ {page} é¡µè¯„ä»·: {page_url}")
                
                # è·å–å½“å‰é¡µé¢
                response = session.get(page_url, timeout=15)
                if response.status_code != 200:
                    self.logger.warning(f"ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    break
                
                # è§£æå½“å‰é¡µé¢
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¯„ä»·å†…å®¹
                review_units = soup.select("li.dcd-review__unit")
                if not review_units:
                    self.logger.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°è¯„ä»·ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                self.logger.info(f"ç¬¬ {page} é¡µæ‰¾åˆ° {len(review_units)} æ¡è¯„ä»·")
                
                # å¤„ç†å½“å‰é¡µé¢çš„è¯„ä»·
                for unit in review_units:
                    try:
                        # æå–è¯„ä»·æ ‡é¢˜
                        title_elem = unit.select_one("span.dcd-review__unit__title")
                        title = title_elem.get_text().strip() if title_elem else ""
                        
                        # æå–è¯„ä»·å†…å®¹ - ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•
                        comment_parts = []
                        
                        # 1. é¦–å…ˆæå–æ‰€æœ‰å¯è§çš„comment divï¼ˆè¿™æ˜¯æœ€å¯é çš„æ–¹æ³•ï¼‰
                        comment_elems = unit.select("div.dcd-review__unit__comment")
                        for comment_elem in comment_elems:
                            # å¤„ç†HTMLä¸­çš„<br>æ ‡ç­¾ï¼Œè½¬æ¢ä¸ºæ¢è¡Œç¬¦
                            comment_html = str(comment_elem)
                            # å°†<br>å’Œ<br/>æ ‡ç­¾æ›¿æ¢ä¸ºæ¢è¡Œç¬¦
                            import re
                            comment_html = re.sub(r'<br\s*/?>', '\n', comment_html)
                            # ç„¶åæå–æ–‡æœ¬å†…å®¹
                            comment_text = BeautifulSoup(comment_html, 'html.parser').get_text().strip()
                            
                            # åªä¿ç•™å®é™…çš„è¯„ä»·å†…å®¹ï¼Œè¿‡æ»¤æ‰è­¦å‘Šå’Œå¯¼èˆªä¿¡æ¯
                            if (comment_text and 
                                len(comment_text) > 20 and  # å†…å®¹è¶³å¤Ÿé•¿
                                not comment_text.startswith("â€»ã“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ä½œå“ã®å†…å®¹ã«é–¢ã™ã‚‹è¨˜è¿°ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚") and
                                "ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹" not in comment_text and
                                "å‚è€ƒã«ãªã‚Šã¾ã—ãŸã‹" not in comment_text and
                                "é•åã‚’å ±å‘Šã™ã‚‹" not in comment_text and
                                "æŠ•ç¥¨ã—ã¦ã„ã¾ã™" not in comment_text and
                                "ã“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å‚è€ƒã«ãªã‚Šã¾ã—ãŸã‹" not in comment_text and
                                "ä¸é©åˆ‡ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å ±å‘Šã™ã‚‹" not in comment_text and
                                "ä»¥ä¸‹ã®å†…å®¹ã«è©²å½“ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å ±å‘Šã§ãã¾ã™" not in comment_text and
                                "å€‹äººæƒ…å ±ã®å…¬é–‹ãƒ»æ¼æ´©" not in comment_text and
                                "ç‰¹å®šã®å€‹äººã‚„ä¼æ¥­ç­‰ã¸ã®å«ŒãŒã‚‰ã›" not in comment_text and
                                "å·®åˆ¥çš„ãªè¡¨ç¾ã®ä½¿ç”¨" not in comment_text and
                                "ç„¡é–¢ä¿‚ãªå®£ä¼ã‚¹ãƒ‘ãƒ " not in comment_text and
                                "æ˜ã‚‰ã‹ã«äº‹å®Ÿã¨ç•°ãªã‚‹è™šå½ã®ä¸»å¼µ" not in comment_text and
                                "å ±å‘Šå¾Œã€å†…å®¹ã‚’ç¢ºèªã—" not in comment_text and
                                "ã‚ˆã‚Šè‰¯ã„ã‚µãƒ¼ãƒ“ã‚¹ç’°å¢ƒã®ãŸã‚" not in comment_text and
                                "ã‚­ãƒ£ãƒ³ã‚»ãƒ«" not in comment_text and
                                "å ±å‘Šã™ã‚‹" not in comment_text and
                                "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ" not in comment_text and
                                "å†åº¦æ™‚é–“ã‚’ãŠã„ã¦ãŠè©¦ã—ãã ã•ã„" not in comment_text and
                                "è³¼å…¥ãƒ»åˆ©ç”¨æ¸ˆã¿" not in comment_text and
                                "ãƒ“ãƒ‡ã‚ª(å‹•ç”»)" not in comment_text):
                                comment_parts.append(comment_text)
                        
                        # 2. å¦‚æœæ ‡å‡†æ–¹æ³•æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æŸ¥æ‰¾å¯èƒ½è¢«æŠ˜å çš„å†…å®¹
                        if not comment_parts:
                            # æŸ¥æ‰¾å¯èƒ½åŒ…å«è¯„ä»·å†…å®¹çš„divï¼Œä½†æ’é™¤å·²çŸ¥çš„å¯¼èˆªå…ƒç´ 
                            excluded_classes = [
                                'dcd-review__unit__bottom',
                                'dcd-review__unit__voted', 
                                'dcd-review__unit__evaluate',
                                'dcd-review__unit__report',
                                'dcd-review__report-modal',
                                'dcd-review__modtogglelink-open'
                            ]
                            
                            # æŸ¥æ‰¾æ‰€æœ‰divï¼Œä½†æ’é™¤å¯¼èˆªç›¸å…³çš„
                            all_divs = unit.select("div")
                            for div in all_divs:
                                # æ£€æŸ¥divçš„classæ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
                                div_classes = div.get("class", [])
                                if any(excluded_class in div_classes for excluded_class in excluded_classes):
                                    continue
                                    
                                # å¤„ç†HTMLä¸­çš„<br>æ ‡ç­¾ï¼Œè½¬æ¢ä¸ºæ¢è¡Œç¬¦
                                div_html = str(div)
                                div_html = re.sub(r'<br\s*/?>', '\n', div_html)
                                div_text = BeautifulSoup(div_html, 'html.parser').get_text().strip()
                                
                                if (div_text and 
                                    len(div_text) > 30 and  # å†…å®¹è¶³å¤Ÿé•¿
                                    "ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹" not in div_text and
                                    "å‚è€ƒã«ãªã‚Šã¾ã—ãŸã‹" not in div_text and
                                    "é•åã‚’å ±å‘Šã™ã‚‹" not in div_text and
                                    "æŠ•ç¥¨ã—ã¦ã„ã¾ã™" not in div_text and
                                    "ã“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å‚è€ƒã«ãªã‚Šã¾ã—ãŸã‹" not in div_text and
                                    "ä¸é©åˆ‡ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å ±å‘Šã™ã‚‹" not in div_text and
                                    "ä»¥ä¸‹ã®å†…å®¹ã«è©²å½“ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å ±å‘Šã§ãã¾ã™" not in div_text and
                                    "å€‹äººæƒ…å ±ã®å…¬é–‹ãƒ»æ¼æ´©" not in div_text and
                                    "ç‰¹å®šã®å€‹äººã‚„ä¼æ¥­ç­‰ã¸ã®å«ŒãŒã‚‰ã›" not in div_text and
                                    "å·®åˆ¥çš„ãªè¡¨ç¾ã®ä½¿ç”¨" not in div_text and
                                    "ç„¡é–¢ä¿‚ãªå®£ä¼ã‚¹ãƒ‘ãƒ " not in div_text and
                                    "æ˜ã‚‰ã‹ã«äº‹å®Ÿã¨ç•°ãªã‚‹è™šå½ã®ä¸»å¼µ" not in div_text and
                                    "å ±å‘Šå¾Œã€å†…å®¹ã‚’ç¢ºèªã—" not in div_text and
                                    "ã‚ˆã‚Šè‰¯ã„ã‚µãƒ¼ãƒ“ã‚¹ç’°å¢ƒã®ãŸã‚" not in div_text and
                                    "ã‚­ãƒ£ãƒ³ã‚»ãƒ«" not in div_text and
                                    "å ±å‘Šã™ã‚‹" not in div_text and
                                    "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ" not in div_text and
                                    "å†åº¦æ™‚é–“ã‚’ãŠã„ã¦ãŠè©¦ã—ãã ã•ã„" not in div_text and
                                    "è³¼å…¥ãƒ»åˆ©ç”¨æ¸ˆã¿" not in div_text and
                                    "ãƒ“ãƒ‡ã‚ª(å‹•ç”»)" not in div_text and
                                    not div_text.startswith("â€»") and
                                    div_text not in comment_parts):
                                    comment_parts.append(div_text)
                        
                        # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•ä»æ•´ä¸ªunitä¸­æ™ºèƒ½æå–
                        if not comment_parts:
                            unit_text = unit.get_text()
                            lines = unit_text.split('\n')
                            content_lines = []
                            in_content = False
                            
                            for line in lines:
                                line = line.strip()
                                if not line:
                                    continue
                                    
                                # å¼€å§‹æ”¶é›†å†…å®¹ï¼ˆåœ¨æ ‡é¢˜ä¹‹åï¼‰
                                if title and title in line:
                                    in_content = True
                                    continue
                                    
                                # åœæ­¢æ”¶é›†å†…å®¹ï¼ˆé‡åˆ°è¯„ä»·è€…ä¿¡æ¯æˆ–å¯¼èˆªå…ƒç´ ï¼‰
                                if (reviewer and reviewer in line) or \
                                   "æŠ•ç¥¨ã—ã¦ã„ã¾ã™" in line or \
                                   "å‚è€ƒã«ãªã‚Šã¾ã—ãŸã‹" in line or \
                                   "é•åã‚’å ±å‘Šã™ã‚‹" in line:
                                    break
                                    
                                # æ”¶é›†å†…å®¹è¡Œ
                                if in_content and len(line) > 10:
                                    content_lines.append(line)
                            
                            if content_lines:
                                comment_parts.append('\n'.join(content_lines))
                        
                        # åˆå¹¶æ‰€æœ‰è¯„è®ºå†…å®¹ï¼Œå»é‡å¹¶ä¿æŒé¡ºåº
                        seen = set()
                        unique_parts = []
                        for part in comment_parts:
                            if part not in seen and len(part.strip()) > 10:
                                seen.add(part)
                                unique_parts.append(part)
                        
                        comment = "\n\n".join(unique_parts) if unique_parts else ""
                        
                        # æå–è¯„åˆ†
                        rating_elem = unit.select_one("span[class*='dcd-review-rating']")
                        rating = ""
                        if rating_elem:
                            class_name = " ".join(rating_elem.get("class", []))
                            rating_match = re.search(r'dcd-review-rating-(\d+)', class_name)
                            if rating_match:
                                rating_value = int(rating_match.group(1))
                                rating = f"{rating_value/10:.1f}" if rating_value > 0 else ""
                        
                        # æå–è¯„ä»·è€…ä¿¡æ¯
                        reviewer_elem = unit.select_one("span.dcd-review__unit__reviewer a")
                        reviewer = reviewer_elem.get_text().strip() if reviewer_elem else ""
                        
                        # æå–å‘å¸ƒæ—¥æœŸ
                        date_elem = unit.select_one("span.dcd-review__unit__postdate")
                        post_date = date_elem.get_text().strip() if date_elem else ""
                        
                        # åªæœ‰å½“æ ‡é¢˜æˆ–å†…å®¹å­˜åœ¨æ—¶æ‰æ·»åŠ 
                        if title or comment:
                            review_data = {
                                "title": title,
                                "comment": comment,
                                "rating": rating,
                                "reviewer": reviewer,
                                "post_date": post_date
                            }
                            all_user_reviews.append(review_data)
                            
                    except Exception as e:
                        self.logger.warning(f"è§£æå•ä¸ªè¯„ä»·æ—¶å‡ºé”™: {str(e)}")
                        continue
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                # æŸ¥æ‰¾åˆ†é¡µé“¾æ¥ï¼Œçœ‹æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                next_page_link = soup.select_one("li a[href*='paging=']")
                if not next_page_link or page >= max_pages:
                    self.logger.info(f"æ²¡æœ‰æ›´å¤šé¡µé¢æˆ–è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                page += 1
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                import time
                time.sleep(1)
            
            self.logger.info(f"æˆåŠŸè·å– {len(all_user_reviews)} æ¡ç”¨æˆ·è¯„ä»·ï¼ˆå…± {page-1} é¡µï¼‰")
            return all_user_reviews
            
        except Exception as e:
            self.logger.error(f"è·å–ç”¨æˆ·è¯„ä»·å¤±è´¥: {str(e)}")
            return []

    def _build_video_dmm_id(self, movie_id):
        """æ„é€  video.dmm.co.jp ä½¿ç”¨çš„IDï¼ˆå¦‚ cosx00087ï¼‰"""
        label, number, _ = self.clean_movie_id(movie_id)
        if not label:
            return None
        return f"{label.lower()}{number.zfill(5)}"

    def _video_dmm_url(self, content_id):
        """æ„é€  video.dmm.co.jp è¯¦æƒ…é¡µ URL"""
        return f"https://video.dmm.co.jp/av/content/?id={content_id}"

    def _extract_content_id_from_video_url(self, url):
        """ä» video.dmm.co.jp é“¾æ¥ä¸­æå– id=XXXX å‚æ•°"""
        try:
            m = re.search(r'[?&]id=([^&#]+)', url, re.IGNORECASE)
            if m:
                return m.group(1)
            return None
        except Exception:
            return None

    def _fetch_video_dmm_content_by_content_id(self, content_id, movie_id=None):
        """ä½¿ç”¨å·²çŸ¥ content_idï¼ˆä¾‹å¦‚ 1stcv00580ã€h_1732orecs00387ï¼‰è°ƒç”¨ GraphQL

        æ›´æ–°è¯´æ˜ (2026-01-27):
        FANZA æ›´æ–°äº† GraphQL APIï¼Œæ—§çš„å¤æ‚æŸ¥è¯¢å·²å¤±æ•ˆã€‚
        ç°åœ¨ä½¿ç”¨ç®€åŒ–çš„æŸ¥è¯¢ç»“æ„ï¼Œç›´æ¥æŸ¥è¯¢æ‰€éœ€å­—æ®µã€‚
        """
        try:
            if not content_id:
                return None

            graphql_url = "https://api.video.dmm.co.jp/graphql"

            # ç®€åŒ–çš„ GraphQL æŸ¥è¯¢ - 2026-01-27 æ›´æ–°
            query = """
query GetContent($id: ID!) {
  ppvContent(id: $id) {
    id
    floor
    title
    isExclusiveDelivery
    releaseStatus
    description
    deliveryStartDate
    makerReleasedAt
    duration
    contentType
    relatedWords
    packageImage {
      largeUrl
      mediumUrl
    }
    sampleImages {
      number
      imageUrl
      largeImageUrl
    }
    actresses {
      id
      name
      imageUrl
    }
    directors {
      id
      name
    }
    series {
      id
      name
    }
    maker {
      id
      name
    }
    label {
      id
      name
    }
    genres {
      id
      name
    }
    makerContentId
  }
  reviewSummary(contentId: $id) {
    average
    total
    withCommentTotal
  }
}
"""

            payload = {
                "operationName": "GetContent",
                "query": query,
                "variables": {"id": content_id}
            }

            session = self.create_session()
            session.headers.update({
                'Accept': 'application/graphql-response+json, application/json',
                'Content-Type': 'application/json',
                'Origin': 'https://video.dmm.co.jp',
                'Referer': self._video_dmm_url(content_id),
                'Sec-Fetch-Site': 'same-site',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Dest': 'empty',
                'Fanza-Device': 'BROWSER',
                'Accept-Language': session.headers.get('Accept-Language', 'ja,en-US;q=0.9,en;q=0.8'),
                'Accept-Encoding': 'gzip, deflate, br, zstd'
            })

            resp = session.post(graphql_url, data=json.dumps(payload), timeout=20)
            if resp.status_code != 200:
                self.logger.warning(f"GraphQL è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
                return None

            data = resp.json()
            if isinstance(data, dict) and data.get('errors'):
                self.logger.warning(f"GraphQLè¿”å›é”™è¯¯: {data.get('errors')}")
                return None

            content = (data or {}).get('data', {}).get('ppvContent')
            if not content:
                self.logger.info("GraphQL æ— å†…å®¹è¿”å›")
                return None

            url = self._video_dmm_url(content_id)
            result = {
                "source": "fanza",
                "id": movie_id or content_id,
                "url": url,
                "title": content.get("title") or ""
            }

            maker_released_at = content.get("makerReleasedAt") or ""
            if maker_released_at:
                result["release_date"] = maker_released_at.split("T")[0].replace("/", "-")

            duration_sec = content.get("duration")
            if isinstance(duration_sec, int) and duration_sec > 0:
                minutes = str(int(round(duration_sec / 60)))
                result["duration"] = minutes + "åˆ†é’Ÿ"

            # æå–æ¼”å‘˜ä¿¡æ¯ï¼ˆåŒ…å«å¤´åƒï¼‰
            performer_nodes = content.get("performers") or content.get("actresses") or []
            if performer_nodes:
                actresses = []
                actress_details = []
                for performer in performer_nodes:
                    if not isinstance(performer, dict):
                        continue
                    name = performer.get("name")
                    if name:
                        actresses.append(name)
                        performer_id = performer.get("id") or performer.get("performerId")
                        avatar_url = performer.get("imageUrl") or performer.get("avatarUrl")
                        actress_info = {
                            "name": name,
                            "id": performer_id or ""
                        }
                        if not avatar_url and performer_id:
                            avatar_url = f"https://pics.dmm.co.jp/mono/actjpgs/{performer_id}.jpg"
                        if avatar_url:
                            actress_info["avatar"] = avatar_url
                        actress_details.append(actress_info)
                if actresses:
                    result["actresses"] = actresses
                    result["actress_details"] = actress_details

            directors_data = content.get("directors") or []
            if directors_data:
                director_names = [
                    d.get("name") for d in directors_data if isinstance(d, dict) and d.get("name")
                ]
                if director_names:
                    if len(director_names) == 1:
                        result["director"] = director_names[0]
                    else:
                        result["director"] = director_names

            series_info = content.get("series")
            if isinstance(series_info, dict):
                series_name = series_info.get("name")
                if series_name:
                    result["series"] = series_name
            elif isinstance(series_info, list) and series_info:
                series_names = [s.get("name") for s in series_info if isinstance(s, dict) and s.get("name")]
                if series_names:
                    result["series"] = series_names[0] if len(series_names) == 1 else series_names

            content_floor = content.get("floor")
            if content_floor:
                result["floor"] = content_floor

            release_status = content.get("releaseStatus")
            if release_status:
                result["release_status"] = release_status

            delivery_start = content.get("deliveryStartDate")
            if delivery_start:
                result["delivery_start_date"] = delivery_start.split("T")[0].replace("/", "-")

            is_exclusive = content.get("isExclusiveDelivery")
            if is_exclusive is not None:
                result["is_exclusive_delivery"] = is_exclusive

            content_type = content.get("contentType")
            if content_type:
                result["content_type"] = content_type

            related_words = content.get("relatedWords")
            if related_words:
                result["related_words"] = related_words

            maker = (content.get("maker") or {}).get("name")
            if maker:
                result["maker"] = maker
            label_name = (content.get("label") or {}).get("name")
            if label_name:
                result["label"] = label_name

            maker_content_id = content.get("makerContentId")
            if maker_content_id:
                result["product_code"] = maker_content_id

            genres = [g.get("name") for g in (content.get("genres") or []) if g.get("name")]
            if genres:
                result["genres"] = genres

            package_image = content.get("packageImage") or {}
            cover = package_image.get("largeUrl") or package_image.get("mediumUrl")
            if cover:
                result["cover"] = cover

            thumbs = []
            for img in (content.get("sampleImages") or []):
                large = img.get("largeImageUrl") or img.get("imageUrl")
                if large:
                    thumbs.append(large)
            if thumbs:
                result["thumbnails"] = thumbs

            review = (data or {}).get('data', {}).get('reviewSummary') or {}
            average = review.get('average')
            if average is not None:
                result["rating"] = str(average)

            total_reviews = review.get('total')
            if total_reviews is not None:
                result["rating_count"] = total_reviews

            with_comment_total = review.get('withCommentTotal')
            if with_comment_total is not None:
                result["rating_with_comment_total"] = with_comment_total

            desc = content.get("description")
            if desc:
                result["summary"] = re.sub(r'<br\s*/?>', '\n', desc).strip()

            # è·å–ç”¨æˆ·è¯„ä»·
            user_reviews = self._fetch_video_dmm_user_reviews(content_id)
            if user_reviews:
                result["user_reviews"] = user_reviews
                # å°†ç”¨æˆ·è¯„ä»·ä¹Ÿæ·»åŠ åˆ°summaryä¸­ï¼ˆå‘åå…¼å®¹ï¼‰
                review_texts = []
                for review in user_reviews:
                    review_text = f"ã€{review.get('title', '')}ã€‘\n{review.get('comment', '')}"
                    review_texts.append(review_text)

                if review_texts:
                    if result.get("summary"):
                        result["summary"] += "\n\n" + "\n\n".join(review_texts)
                    else:
                        result["summary"] = "\n\n".join(review_texts)

            self.logger.info("é€šè¿‡ GraphQL æˆåŠŸè·å–è¯¦æƒ…ï¼ˆcontent_idï¼‰")
            return result

        except requests.exceptions.RequestException as e:
            self.logger.error(f"GraphQL è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"GraphQL è§£æå¼‚å¸¸: {str(e)}")
            return None

    def _fetch_video_dmm_user_reviews(self, content_id):
        """ä»video.dmm.co.jpè·å–ç”¨æˆ·è¯„ä»·"""
        try:
            # æ„å»ºç”¨æˆ·è¯„ä»·GraphQLæŸ¥è¯¢
            query = """
            query UserReviews($id: ID!, $sort: ReviewSort!, $offset: Int!) {
                reviews(contentId: $id, sort: $sort, limit: 10, offset: $offset) {
                    items {
                        id
                        title
                        rating
                        reviewerId
                        nickname
                        isPurchased
                        comment
                        helpfulCount
                        service
                        isExposure
                        publishDate
                        __typename
                    }
                    __typename
                }
            }
            """
            
            variables = {
                "id": content_id,
                "offset": 0,
                "sort": "HELPFUL_COUNT_DESC"
            }
            
            payload = {
                "operationName": "UserReviews",
                "query": query,
                "variables": variables
            }
            
            # å‘é€GraphQLè¯·æ±‚
            session = self.create_session()
            session.headers.update({
                'Accept': 'application/graphql-response+json, application/json',
                'Content-Type': 'application/json',
                'Origin': 'https://video.dmm.co.jp',
                'Referer': self._video_dmm_url(content_id)
            })
            
            response = session.post(
                "https://api.video.dmm.co.jp/graphql",
                data=json.dumps(payload),
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                if "data" in data and "reviews" in data["data"] and "items" in data["data"]["reviews"]:
                    reviews = data["data"]["reviews"]["items"]
                    self.logger.info(f"æˆåŠŸä»video.dmm.co.jpè·å– {len(reviews)} æ¡ç”¨æˆ·è¯„ä»·: {content_id}")
                    
                    # è½¬æ¢æ•°æ®æ ¼å¼ä»¥åŒ¹é…ç°æœ‰ç»“æ„
                    user_reviews = []
                    for review in reviews:
                        # å¤„ç†æ—¥æœŸæ ¼å¼
                        publish_date = review.get("publishDate", "")
                        if publish_date:
                            # è½¬æ¢ISOæ ¼å¼æ—¥æœŸä¸ºç®€å•æ ¼å¼
                            try:
                                from datetime import datetime
                                dt = datetime.fromisoformat(publish_date.replace('Z', '+00:00'))
                                publish_date = dt.strftime("%Y-%m-%d")
                            except:
                                publish_date = publish_date[:10] if len(publish_date) >= 10 else publish_date
                        
                        review_data = {
                            "title": review.get("title", ""),
                            "comment": review.get("comment", ""),
                            "rating": str(review.get("rating", 0)),
                            "reviewer": review.get("nickname", ""),
                            "post_date": publish_date,
                            "helpful_count": review.get("helpfulCount", 0),
                            "is_purchased": review.get("isPurchased", False),
                            "service": review.get("service", "")
                        }
                        user_reviews.append(review_data)
                    
                    return user_reviews
                else:
                    self.logger.warning(f"GraphQLå“åº”ä¸­æœªæ‰¾åˆ°reviewsæ•°æ®: {content_id}")
                    return []
            else:
                self.logger.warning(f"ç”¨æˆ·è¯„ä»·GraphQLè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"ä»video.dmm.co.jpè·å–ç”¨æˆ·è¯„ä»·å¤±è´¥: {str(e)}")
            return []

    def _fetch_video_dmm_content(self, movie_id):
        """åŸºäº movie_id æ¨å¯¼ content_id åè°ƒç”¨ GraphQL"""
        content_id = self._build_video_dmm_id(movie_id)
        return self._fetch_video_dmm_content_by_content_id(content_id, movie_id)
